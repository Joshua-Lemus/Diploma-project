# SET MAIN PARAMS

pars = Pars()

## random task name
# pars.dataset = "RANDOM"
# pars.dataset = "RANDOM-Y"
pars.dataset = "NLGP"
# pars.dataset = "RANDOM-FF-Teacher"
# pars.dataset = "RANDOM-FF-Teacher-Y"
# pars.dataset = "RANDOM-RNN-Teacher"
# pars.dataset = "RANDOM-RNN-Teacher-X"
# pars.dataset = "RANDOM-RNN-Teacher-Y"

## dataset name
# pars.dataset = "MNIST1D"
# pars.dataset = "MNIST"
# pars.dataset = "MNIST10"
# pars.dataset = "FashionMNIST"
# pars.dataset = "CIFAR10"
# pars.dataset = "CIFAR100"

## random dataset specific options
pars.exp_input = False # exponentially distributed input (default is gaussian)
pars.rho = 0. # input covariance for RANDOM dataset

## teacher-student FF options
pars_teacher = {}
# pars_teacher["K"] = 1 # number of hidden units in the random teacher
# pars_teacher["nonlinearity"] = "linear" # activation function : sigmoid | tanh | erf | relu | linear
# pars_teacher["g_bias_in"] = 0. # gain for biases initialization
# pars_teacher["soft_w"] = True # constant output weights
# pars_teacher["num_out"] = 1 # number of output units 
# pars_teacher["g"] = 1. # gain of ff weights
# pars_teacher["J"] = None # optionally make your own kind of teacher
# pars.pars_teacher = pars_teacher

# ## teacher-student RNN options
# pars_teacher = {}
# pars_teacher["K"] = 40 # number of hidden units in the random teacher
# pars_teacher["nonlinearity"] = "relu" # activation function : sigmoid | tanh | erf | relu | linear
# pars_teacher["discrete_time"] = False # type of temporal dynamics
# pars_teacher["dt"] = 0.1 # time discretization
# pars_teacher["eye_in"] = True # identity input weights
# pars_teacher["g_bias_in"] = 0. # gain for biases initialization
# pars_teacher["soft_w"] = False # constant output weights
# pars_teacher["num_out"] = 1 # number of output units 
# pars_teacher["g"] = 0.8 # gain of recurrent weights
# pars_teacher["J"] = None # optionally make your own kind of teacher
# pars.pars_teacher = pars_teacher

## NLGP specific options
pars.torus = True # whether to use a torus topology
pars.g_nonlin = 1e-10 # gain parameter for nonlinearity
pars.xis = [2.3] # lenght-scales of mixtures

## spatial options
pars.dim = 1 # input dimension
pars.lN =100 # linear input size
pars.normalize = False # normalize inputs (some dataset are already normalized)

## misc options
pars.seed_data = 10
pars.data_dir = f'data'
pars.save_dir = f'results_notebook'

# # TO DIRECTLY DOWNLOAD MNIST1D USE THE FOLLOWING:
# import requests
# url = 'https://github.com/greydanus/mnist1d/raw/master/mnist1d_data.pkl'
# r = requests.get(url, allow_redirects=True)
# save_dir_dataset = save_dir + "/" + dataset
# open(f'{data_dir}/mnist1d_data.pkl', 'wb').write(r.content)

# General options
pars.alpha_train = 0.2 # fraction of training patterns wrt N in a random task
pars.alpha_test = 2/pars.lN # fraction of test patterns wrt N in a random task
pars.span_h = False # generate all h in range (only makes sense for N=2 or N=3)




lN, N = pars.lN, pars.N            # Size of the patterns to be learned (lN for linear dimension, N for total dimension)
P = pars.num_train                 # Number of patterns to learn
fe = -1                            # Fraction of excitatory connections
sparsity_mean = 0.                 # If you use sparsity instead of sign(train_dataset), you'll generate a sequence 
#                                  # which will be later give the probability to have a 0 in X
sparsity_std = 0.                  # To control std for sparsity
noself = True                      # Set diagonal to 0, to prevent trivial solution J=id(NxN)
use_bias = False                    # Use a bias (I, input current): H = JX + I
learn_bias = True                  # Learn bias or not (fix it instead)
bias = 0.                          # Initialize the bias 
norm_dependent_stability = False   # Stability is proportional to ||J^2|| (otherwise it's proportional to sqrt(N))
embed_model = False                 # Embed the model to a graph
random_pos = True                 # Embeded positions are random
dim = pars.dim                     # Dimension of the data (it's already previously defined)
torus = pars.torus                 # Whether to use a torus topology (alreade defined previously)


# learning options
num_epochs = 200          # Number of epochs
print_every = 10          # Verbose option
lr = 0.1                 # Learning rate
delta0 = 0.1              # Stability regularization
gam = 0.                  # Regularization strength (L1?)
gam_en = 0.               # Energy regularization
gam_sign = 0.05            # Sign regularization
gam_pos = 0.0              # Position regularization
relu2 = True             # Whether to use relu or relu^2 as activation function 
maxweight = 5           # Don't let weight be too large
square_dist = False       # Whether to set distance as (xi^2+xj^2) or as np.abs(xi^2+xj^2)
stop_at_zero_err = False  # You may want to continue training after 0 error because sign regularization can still improve
