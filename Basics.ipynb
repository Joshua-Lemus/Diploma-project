{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ce0946-00c8-4b4b-a1d9-87b6015dce73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, datetime\n",
    "import copy\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# my own\n",
    "from tasks import get_data\n",
    "from myutils import *\n",
    "from topology import gen_lattice, compute_dist\n",
    "from scipy.stats import expon, multivariate_normal\n",
    "\n",
    "# increase cell width\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# SOME USEFUL DEFINITIONS\n",
    "relu = lambda x : (x > 0) * x\n",
    "def pr(h, b):\n",
    "    if b > 1e2:\n",
    "        return 1. * (h > 0)\n",
    "    else:\n",
    "        return np.exp(b * h) / (2 * np.cosh(b * h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c2b922-9067-44c1-9010-3efae5d504bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = multivariate_normal.rvs(mean=np.zeros(20), cov=0.1, size=5)\n",
    "# y = np.random.randint(3, size = 5)\n",
    "# print(torch.tensor(y[5:]))\n",
    "\n",
    "#print(int(pars.alpha_test * pars.lN))\n",
    "# a=len([1,3])\n",
    "# b=[None]*a\n",
    "# print(a, b)\n",
    "\n",
    "# ['a']*2\n",
    "# 1-True\n",
    "# d=5\n",
    "# positions = torch.arange(d, dtype=torch.float)\n",
    "# distances = positions[None] - positions[:, None]\n",
    "# print(positions[None])\n",
    "# print(positions[:,None])\n",
    "# print(distances)\n",
    "# b=torch.minimum(torch.abs(distances), d- torch.abs(distances))\n",
    "# print(b)\n",
    "\n",
    "# a=np.array([[1,2,3],[3,4,5]])\n",
    "# print(a.shape)\n",
    "# print(np.kron(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952edb4c-84ba-4690-95c9-bae38fce319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MAIN PARAMS\n",
    "\n",
    "# Pars() initializes a Class with definitions for parameters\n",
    "pars = Pars()\n",
    "\n",
    "pars.dataset = \"RANDOM\" #\"NLGP\" #Non-linear Gaussian Process\n",
    "\n",
    "## spatial options\n",
    "pars.dim = 1             # Input dimension\n",
    "pars.normalize = False   # Normalize inputs (some dataset are already normalized)\n",
    "\n",
    "## random dataset options\n",
    "pars.alpha_test = 0.     # Fraction of test patterns wrt N in a random task\n",
    "pars.span_h = False       # Generate all h in range (only makes sense for N=2 or N=3)\n",
    "pars.rho = 0.             # Input covariance\n",
    "\n",
    "## NLGP specific options\n",
    "pars.g_nonlin = 0#1e-10      # Gain parameter for nonlinearity\n",
    "#pars.xis = [1e-10]           #-# Lenght-scales of mixtures (?)\n",
    "\n",
    "## misc options\n",
    "pars.seed_data = 1                   # Initialize a random seed\n",
    "pars.data_dir = f'data'              # Where data will be saved (it actually is not saved)\n",
    "pars.save_dir = f'results_notebook'  # Where results will be saved (they actually are not solved)\n",
    "\n",
    "makedir(pars.data_dir)\n",
    "makedir(pars.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4140b1-8b92-4ed3-ab37-8c9406dd97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global network options\n",
    "noself = True                      # Neurons don't link with themselves\n",
    "use_bias = False                   # Use a bias/input current\n",
    "learn_bias = False                 # Learn the bias\n",
    "bias = 0.                          # Initial value for the bias\n",
    "norm_dependent_stability = False   #-# The margin inforced for stability of the solution depends on the norm of the weights?\n",
    "dim = pars.dim                     # Dimension of the space where the inputs lives in (a line, R2, R3, etc)\n",
    "\n",
    "# learning options\n",
    "num_epochs = 1000          # Number of epochs\n",
    "print_every = 100          # Verbose option\n",
    "lr = 0.1                  # Learning rate\n",
    "delta0 = 0                #-# Strength for inforcing the margin of stability\n",
    "gam = 0.                  #-# Regularization strength (for not having big weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa6fd47-60fe-4c6e-ad62-9f5106abada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, Y, N, P, noself=True, num_epochs=200, delta0=0.0, bias=0, use_bias=False, learn_bias=False, stop_at_zero_err=True):\n",
    "    # init J: matrix with weight between neurons\n",
    "    #J = np.random.randn(N, N) / np.sqrt(N)      # Initialize J with random values\n",
    "    J = np.zeros((N,N))\n",
    "    diag_indices = np.diag_indices(N)           # The diagonal indices\n",
    "    if noself:                                  # Optionally, set diagonal to 0\n",
    "        J[diag_indices] = 0.                    \n",
    "    J0 = J.copy()                               # Save state of J before training\n",
    "    \n",
    "    # Init I: bias input matrix\n",
    "    I = bias * np.ones(N) \n",
    "    \n",
    "    # Initialize variables\n",
    "    errs, en_regs, sign_regs, pos_regs = [], [], [], []  # Holders for error estimation\n",
    "    ok = False                                           # Will say when to stop (at convergence) \n",
    "    \n",
    "    # run optimization\n",
    "    for ep in range(num_epochs):\n",
    "    \n",
    "        # run through data (updating units randomly)\n",
    "        perm = np.random.permutation(P) # random dynamical update\n",
    "        for mu in perm:\n",
    "    \n",
    "            # compute fields\n",
    "            H = J @ X[mu]               # Weights*data\n",
    "            if use_bias:                # Optionally, pdate the bias\n",
    "                H += I                   \n",
    "    \n",
    "            # learning step\n",
    "            delta = delta0 * np.sqrt((J**2).sum(-1)) if norm_dependent_stability else delta0 * np.sqrt(N)  # Margin for stability\n",
    "            err = Y[mu] * ((Y[mu] * H) <= 0)                 # \n",
    "            derr = np.outer(err,X[mu])\n",
    "            J += lr * derr\n",
    "            #J -= lr * gam * J\n",
    "    \n",
    "            # train input bias\n",
    "            if use_bias and learn_bias:\n",
    "                I += lr * err\n",
    "    \n",
    "            # apply energy, sign and position regularization; enforce weight constraings\n",
    "            if noself:                                  # Optionally, set diagonal to 0\n",
    "                J[diag_indices] = 0.                    \n",
    "        \n",
    "        # compute errors and print\n",
    "        if ep % print_every == 0:\n",
    "    \n",
    "            # compute loss on entire training set\n",
    "            Hs = X @ J.T\n",
    "            if use_bias:\n",
    "                Hs += I[None]\n",
    "            err = ((Hs * Y) <= delta).mean()\n",
    "            errs += [err]\n",
    "    \n",
    "            # compute energy, sign and position regularization error\n",
    "    \n",
    "            # print info\n",
    "            #toprint = f'ep: {ep} loss: {err}'\n",
    "            #print(toprint)\n",
    "    \n",
    "            # optionally stop at convergence, print epoch of convergence\n",
    "            if err.sum() == 0:\n",
    "                if not ok:\n",
    "                    ep_conv = ep\n",
    "                    ok = True\n",
    "                if stop_at_zero_err:\n",
    "                    break\n",
    "    \n",
    "    if not ok:\n",
    "        print(f'loss not zero in {ep} epochs - min av error {errs[-1]}')\n",
    "        return errs, errs[-1], J, J0\n",
    "    else:\n",
    "        print(f'zero loss converged at {ep_conv} epochs')\n",
    "        return errs, errs[-1], J, J0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e435c28-3681-42fe-bdc5-c1cf5ac732d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### N=1000\n",
      "alpha=1.7\n",
      "loss not zero in 999 epochs - min av error 4.823529411764706e-05\n",
      "alpha=2\n",
      "loss not zero in 999 epochs - min av error 0.1311065\n",
      "alpha=2.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#print(P/N)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     N \u001b[38;5;241m=\u001b[39m pars\u001b[38;5;241m.\u001b[39mN       \u001b[38;5;66;03m# linear and total input dimension\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     errs, err, J,J0 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoself\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                                \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_at_zero_err\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(err)\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(alpha_span, errors, label\u001b[38;5;241m=\u001b[39mN)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(X, Y, N, P, noself, num_epochs, delta0, bias, use_bias, learn_bias, stop_at_zero_err)\u001b[0m\n\u001b[1;32m     21\u001b[0m perm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(P) \u001b[38;5;66;03m# random dynamical update\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mu \u001b[38;5;129;01min\u001b[39;00m perm:\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# compute fields\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mJ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m]\u001b[49m               \u001b[38;5;66;03m# Weights*data\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_bias:                \u001b[38;5;66;03m# Optionally, pdate the bias\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         H \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m I                   \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train for different values of alpha\n",
    "delta0 = 0#0.001\n",
    "N_span=[1000, 1500]\n",
    "alpha_span = [1.7, 2, 2.3]#, 2.6]#np.linspace(0.1,3,6)\n",
    "for sample_dimension in N_span:#, 500, 1000, 2000]:\n",
    "    print(f'######################### N={sample_dimension}')\n",
    "    pars.lN = sample_dimension            # Linear input dimension\n",
    "    errors = []\n",
    "    for alpha_train in alpha_span:\n",
    "        pars.alpha_train = alpha_train     # Fraction of training patterns wrt N in a random task\n",
    "        \n",
    "        # GET DATASET\n",
    "        data_and_properties = get_data(pars, dtype=dtype, device=device)\n",
    "        train_dataset, test_dataset, teacher_weights, x_teacher = data_and_properties\n",
    "        # generatae patterns of -1s and 1s\n",
    "        X = np.sign(train_dataset.tensors[0].numpy()) #0.5 * (1 + np.sign(train_dataset.tensors[0].numpy())) #\n",
    "        Y = X #2*X-1#train_dataset.tensors[0].numpy()#np.sign(X)\n",
    "\n",
    "        print(f\"alpha={alpha_train}\") #Verbose option\n",
    "        #print(f\"num train: {pars.num_train}, num test: {pars.num_test}, num label: {pars.num_label}\")\n",
    "        P = pars.num_train            # Number of inputs \n",
    "        #print(P/N)\n",
    "        N = pars.N       # linear and total input dimension\n",
    "    \n",
    "        errs, err, J,J0 = train_network(X, Y, N, P, noself=True, num_epochs=num_epochs, delta0=delta0, \n",
    "                                    use_bias=use_bias, bias=0, learn_bias=learn_bias, stop_at_zero_err=True)\n",
    "        errors.append(err)\n",
    "    \n",
    "    plt.plot(alpha_span, errors, label=N)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('error')\n",
    "    plt.title(f'Critcal capacity for N up to {N}')\n",
    "    plt.savefig(f'{pars.save_dir}/Critical-{N}')\n",
    "    plt.legend()\n",
    "    #plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8669d2b-32f6-41b0-892e-568b0563f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=J@X[1]\n",
    "err=X[1]*((Y[1]*h) <= 0)\n",
    "print(err)\n",
    "print(X[1])\n",
    "print(np.outer(err,X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a680e1-7f17-42e0-950c-28b74a6f1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(int(alpha_train*N)):\n",
    "    incorrect = (np.sign(J@X[m])!=X[m]).sum(-1)\n",
    "    if incorrect >0:\n",
    "        print(incorrect)\n",
    "\n",
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e31641-6e0d-4111-9fe6-df6927093d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be781a-4074-401c-8437-638f4de0f590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
