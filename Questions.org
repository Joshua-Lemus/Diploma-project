- [ ] I get the general idea of how the data builder works, but still not sure on what it's doing or what it's returning.
  - Using on pars (particularly, depending on /pars.dataset/), we build /data_and_properties/ by calling /get_data()/ +which is in tasks.py+
  - get_data() calls a specialized function depending on the dataset we're useing (eg: if pars.datasets=="NLGP", we call /get_data_nlgp()/ +still inside tasks.py+)
    - This function creates the data through another special function, eg for NLGP, it calls /Mixture(distributions)/; /distributions/ is a set of distributions created by the function /build_nlgp_mixture()/
      - build_nlgp_mixture() returns a mixture of distributions (eg a number of non-linear-gaussian-processes), each returned by /inputs.NLGP()/ +which is in inputs+
	- Got lost on the following-up, but in the end this is calling many times a random number generator according to a given function (eg the error function)
  - We extract the actual data and call it /train_dataset/ and /text_dataset/
  - Call extract_tensors(train_dataaset) to make sure the data has a pytorch tensor structure
- [ ] What watch_dataset +from myutils.py+ is doing...
  - Depending on the input dimension (pars.dim), it    
- [ ] In /gen_lattice/, when passing dim=1 and centered=True, you get a tensor that goes from 1 to N, but with a step of (N/N+1), I don't get why
- [X] What kind of distance are we computing in /compute_dist()/
- [X] 'In the recurrent reg' graph, what are the lines showing?
- [X] What is X? Why is Y=data?
- [ ] The value of the bias = 0. is intentional so that I =~ 0? (it's not exactly 0 as I'd expected)
- [X] What is delta?
- [ ] What's the equation for the sign regularization?
- [ ] What is np.roll doing? Is it like an average?
- [ ] Why np.ones_like(pos[indI])?
